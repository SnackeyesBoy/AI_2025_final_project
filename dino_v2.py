import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler
from torchvision import transforms
from torchvision.utils import save_image
from transformers import Dinov2Model
from peft import LoraConfig, get_peft_model
import pandas as pd
import os
from PIL import Image
from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score
import time
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# ================= 1. è¶…åƒæ•¸è¨­å®š =================
# è³‡æ–™è·¯å¾‘è¨­å®š (æŒ‡å‘åŒç›®éŒ„ä¸‹çš„è³‡æ–™å¤¾)
DATA_ROOT = "isic-2024-challenge" 
CSV_PATH = os.path.join(DATA_ROOT, "train-metadata.csv")
IMG_FOLDER = os.path.join(DATA_ROOT, "train-image/image")

# è¨“ç·´åƒæ•¸ (é‡å° RTX 2080 Super å„ªåŒ–)
BATCH_SIZE = 16            # 8GB VRAM çš„å®‰å…¨å€¼
EPOCHS = 10                # è¨“ç·´è¼ªæ•¸
SAMPLES_PER_EPOCH = 30000  # ã€é»ƒé‡‘ç­–ç•¥ã€‘æ¯ä¸€è¼ªåªçœ‹ 3 è¬å¼µï¼Œä½†å¾å…¨é‡è³‡æ–™éš¨æ©ŸæŠ½
LR = 1e-4                  # å­¸ç¿’ç‡
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ================= 2. å®šç¾© Dataset èˆ‡ Model Class =================

class ISICLazyDataset(Dataset):
    def __init__(self, df, img_dir, transform=None):
        self.data = df
        self.img_dir = img_dir
        self.transform = transform
    def __len__(self): return len(self.data)
    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        img_name = f"{row['isic_id']}.jpg"
        img_path = os.path.join(self.img_dir, img_name)
        label = int(row['target'])
        try:
            image = Image.open(img_path).convert("RGB")
            if self.transform: image = self.transform(image)
            return image, label
        except Exception as e:
            # è®€å–å¤±æ•—å›å‚³é»‘åœ–ï¼Œé¿å…å´©æ½°
            return torch.zeros(3, 224, 224), label

class SkinClassifier(nn.Module):
    def __init__(self, encoder):
        super().__init__()
        self.encoder = encoder
        self.classifier = nn.Linear(768, 2) # äºŒåˆ†é¡ (è‰¯æ€§/æƒ¡æ€§)

    def forward(self, x):
        outputs = self.encoder(x)
        # å– CLS Token (ç¬¬ 0 å€‹å‘é‡)
        return self.classifier(outputs.last_hidden_state[:, 0, :])

# ================= 3. ä¸»åŸ·è¡Œå€å¡Š (Windows ä¿è­·é–) =================
if __name__ == '__main__':
    print(f"ğŸš€ [1/7] æ­£åœ¨åˆå§‹åŒ–è¨­å®š...")
    print(f"   ä½¿ç”¨è£ç½®: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}")
    
    # --- è·¯å¾‘æª¢æŸ¥ ---
    if not os.path.exists(CSV_PATH):
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° CSV æª”ï¼è·¯å¾‘: {os.path.abspath(CSV_PATH)}")
        exit()
    if not os.path.exists(IMG_FOLDER):
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°åœ–ç‰‡è³‡æ–™å¤¾ï¼è·¯å¾‘: {os.path.abspath(IMG_FOLDER)}")
        exit()

    # --- è®€å–æ•¸æ“š ---
    print("ğŸ“‚ [2/7] è®€å– CSV æ¨™ç±¤æª”...")
    # low_memory=False æ¶ˆé™¤ DtypeWarning
    df_full = pd.read_csv(CSV_PATH, low_memory=False) 
    df_full = df_full[['isic_id', 'target']]

    # åˆ‡åˆ†è¨“ç·´é›†èˆ‡é©—è­‰é›† (å›ºå®š random_state ä»¥ç¢ºä¿å…¬å¹³æ¯”è¼ƒ)
    train_df = df_full.sample(frac=0.8, random_state=42)
    val_df = df_full.drop(train_df.index)

    # --- å½±åƒå‰è™•ç† ---
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomVerticalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    train_dataset = ISICLazyDataset(train_df, IMG_FOLDER, transform=train_transform)
    val_dataset = ISICLazyDataset(val_df, IMG_FOLDER, transform=val_transform)

    # --- ã€é—œéµã€‘å¹³è¡¡æ¡æ¨£ç­–ç•¥ ---
    print("âš–ï¸  [3/7] è¨ˆç®—é¡åˆ¥æ¬Šé‡ (å…¨é‡æ±  + å‹•æ…‹æ¡æ¨£)...")
    targets = train_df['target'].values
    class_counts = [(targets == 0).sum(), (targets == 1).sum()]
    if class_counts[1] == 0: class_counts[1] = 1 # é˜²å‘†
    weight = 1. / torch.tensor(class_counts, dtype=torch.float)
    samples_weight = torch.tensor([weight[t] for t in targets])

    # replacement=True ä»£è¡¨å…è¨±é‡è¤‡æŠ½æ¨£ï¼Œä¿è­‰æ¯ä¸€è¼ª 3 è¬å¼µéƒ½èƒ½çœ‹åˆ°ä¸åŒçš„è‰¯æ€§åœ–ç‰‡
    sampler = WeightedRandomSampler(samples_weight, num_samples=SAMPLES_PER_EPOCH, replacement=True)

    # DataLoader
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4) 

    # --- è¦–è¦ºé©—è­‰ï¼šæª¢æŸ¥æ˜¯å¦æœ‰è®€åˆ°åœ–ç‰‡ ---
    print("ğŸ‘€ [4/7] æ­£åœ¨é€²è¡Œåœ–ç‰‡è®€å–æª¢æŸ¥ (å­˜æª” debug_check.png)...")
    try:
        check_iter = iter(train_loader)
        images, labels = next(check_iter)
        # åå‘æ¨™æº–åŒ–ä»¥ä¾¿è‚‰çœ¼è§€å¯Ÿ
        inv_normalize = transforms.Normalize(
            mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
            std=[1/0.229, 1/0.224, 1/0.225]
        )
        check_imgs = [inv_normalize(img) for img in images]
        save_image(torch.stack(check_imgs), "debug_check.png", nrow=4)
        print("   âœ… æª¢æŸ¥æˆåŠŸï¼è«‹æ‰“é–‹ 'debug_check.png' ç¢ºèªåœ–ç‰‡æ˜¯å¦æ­£å¸¸ã€‚")
    except Exception as e:
        print(f"   âš ï¸ è®€å–æª¢æŸ¥å¤±æ•— (å¯èƒ½æ˜¯è·¯å¾‘å•é¡Œ): {e}")

    # --- è¼‰å…¥æ¨¡å‹ ---
    print("ğŸ¦– [5/7] è¼‰å…¥ DINOv2 (With Registers) + LoRA...")
    base_model = Dinov2Model.from_pretrained("facebook/dinov2-with-registers-base")

    # LoRA è¨­å®šï¼šåªå¾®èª¿ Attention (query, value)ï¼Œæ¥µçœé¡¯å­˜
    peft_config = LoraConfig(
        r=16, lora_alpha=16, target_modules=["query", "value"], 
        lora_dropout=0.1, bias="none"
    )
    model = get_peft_model(base_model, peft_config)
    model.print_trainable_parameters()
    
    model = SkinClassifier(model).to(DEVICE)
    optimizer = optim.AdamW(model.parameters(), lr=LR)
    criterion = nn.CrossEntropyLoss()
    scaler = torch.cuda.amp.GradScaler(enabled=True) # FP16 æ··åˆç²¾åº¦

    history = {'train_loss': [], 'val_acc': [], 'val_f1': []}

    # --- é–‹å§‹è¨“ç·´ ---
    print(f"ğŸ”¥ [6/7] é–‹å§‹æˆ°é¬¥ï¼é è¨ˆè¨“ç·´ {EPOCHS} è¼ª...")
    print("-" * 50)

    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        start_time = time.time()
        
        for i, (imgs, lbls) in enumerate(train_loader):
            imgs, lbls = imgs.to(DEVICE), lbls.to(DEVICE)
            
            optimizer.zero_grad()
            with torch.cuda.amp.autocast(enabled=True):
                outputs = model(imgs)
                loss = criterion(outputs, lbls)
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            total_loss += loss.item()
            
            if i % 50 == 0: 
                print(f"\rEpoch {epoch+1} | Step {i}/{len(train_loader)} | Loss: {loss.item():.4f}", end="")

        avg_loss = total_loss / len(train_loader)
        history['train_loss'].append(avg_loss)
        
        # --- é©—è­‰ ---
        print(f"\n   æ­£åœ¨è¨ˆç®—é©—è­‰åˆ†æ•¸...", end="")
        model.eval()
        all_preds = []
        all_labels = []
        with torch.no_grad():
            for i, (imgs, lbls) in enumerate(val_loader):
                if i > 500: break # åªæ¸¬å‰ 8000 å¼µï¼Œç¯€çœæ™‚é–“
                imgs, lbls = imgs.to(DEVICE), lbls.to(DEVICE)
                outputs = model(imgs)
                _, preds = torch.max(outputs, 1)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(lbls.cpu().numpy())
        
        val_acc = accuracy_score(all_labels, all_preds)
        val_f1 = f1_score(all_labels, all_preds, average='weighted')
        history['val_acc'].append(val_acc)
        history['val_f1'].append(val_f1)
        
        print(f"\râœ… Epoch {epoch+1} çµæŸ ({(time.time()-start_time)/60:.1f} min) | Loss: {avg_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}")
        torch.save(model.state_dict(), f"dino_2080_epoch_{epoch+1}.pth")

    # --- è‡ªå‹•ç¹ªåœ– ---
    print("ğŸ“Š [7/7] æ­£åœ¨ç”Ÿæˆå ±å‘Šåœ–è¡¨...")
    
    # åœ– 1: è¨“ç·´è¶¨å‹¢
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Train Loss', color='red', marker='o')
    plt.title('Training Loss (DINOv2)')
    plt.grid(True); plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history['val_f1'], label='Val F1-Score', color='green', marker='o')
    plt.title('Validation F1-Score (DINOv2)')
    plt.grid(True); plt.legend()
    plt.tight_layout()
    plt.savefig('dino_training_curves.png')

    # åœ– 2: æ··æ·†çŸ©é™£
    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix (DINOv2)')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.savefig('dino_confusion_matrix.png')

    print("ğŸ† å…¨éƒ¨å®Œæˆï¼è«‹æŸ¥çœ‹ 'dino_training_curves.png' èˆ‡ 'dino_confusion_matrix.png'ã€‚")
    print(f"æœ€çµ‚ F1-Score: {history['val_f1'][-1]:.4f}")